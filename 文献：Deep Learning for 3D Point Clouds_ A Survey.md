# 文献阅读笔记：Deep Learning for 3D Point Clouds: A Survey

**三维数据**通常显示的数据格式：

①深度图（depth images）

②点云（point clouds）

③网格（meshes）

④体素网格（volumet ric grids）

‍

**公开数据集**(部分)：

①ModelNet    ②ScanObjectNN    ③ShapeNet    ④PartNet    ⑤S3DIS    ⑥ScanNet    ⑦Semantic3D    ⑧ApolloCar3D    ⑨KITTI Vision Benchmark Suite   

‍

**处理点云的方法**：

###### ①三维形状分类（3D shape classification）

    （1）合成数据集：Model Net [6]、ShapeNet [8]  （数据集物体完整，无遮挡无背景干扰）

    （2）真实场景数据集：ScanObjectNN [7]、ScanNet [11]（数据集物体有遮挡或背景噪声）

###### ②三维目标检测与追踪（3D object detection and tracking）

    （1）室内场景数据集：由稠密深度图转换而来，或三维模型采样获得

    （2）室外城市场景数据集：主要用于自动驾驶，物体通常分离且点云较为稀疏

###### ③三维点云分割（3D point cloud segmentation）

获取数据集的传感器类型：

    （1）可移动激光雷达 / Mobile Laser Scanners (MLS)

    （2）航空激光雷达 / Aerial Laser Scanners (ALS)

    （3）静态地面激光雷达 / static Terrestrial Laser Scanners (TLS)

    （4）RGB-D相机 / RGB-Dcameras

    （5）3D扫描仪 / 3D scanners

###### ④三维点云配准（3D point cloud registration）

###### ⑤六自由度姿态估计（6-DOF pose estimation）

[6-DoF问题相关基础知识笔记_6dof-CSDN博客](https://blog.csdn.net/qq_40270044/article/details/122127707)

###### ⑥三维重建（3D reconstruction）

![image](assets/image-20250724094120-coel4fy.png "三维点云深度学习方法的分类")

‍

**评估指标**（常用）：

###### ①三维形状分类：

    （1）整体准确率 / Overall Accuracy(OA) ：所有测试样本的平均准确率

    （2）平均类别准确率 / meanclassaccu racy (mAcc) ：所有类别的平均准确率

###### ②三维目标检测：

    （1）平均精度 / Average Precision (AP)：通过计算“查准率-查全率”曲线（precision-recall curve）下的面积来计算

    （2）Precision ：评估三维<u>单目标</u>跟踪的整体性能

    （3）Success ：评估三维<u>单目标</u>跟踪的整体性能

    （4）多目标跟踪平均准确率 / Average Multi-Object Tracking Accuracy (AMOTA)：三维<u>多目标</u>跟踪评估中最常用的指标

    （5）多目标跟踪平均精度 / Average Multi-Object Tracking Precision (AMOTP)：三维<u>多目标</u>跟踪评估中最常用的指标

###### ③三维点云分割：

    （1）整体准确率 / Overall Accuracy(OA) ：所有测试样本的平均准确率

    （2）平均类别准确率 / meanclassaccu racy (mAcc)：所有形状类别的平均准确率

    （3）平均交并比 / mean Intersec tion over Union (mIoU)

    （4）平均精度 / mean Average Precision (mAP)：常用于三维点云实例分割的评估

---

### <span data-type="text" style="background-color: var(--b3-card-error-background); color: var(--b3-card-error-color);">三维形状分类：</span>

此方法通常首先学习整个点云的特征输入，再通过聚类方法获得全局形状嵌入。

分类最终通过将嵌入特征传递到多个连接的神经网络层实现。

<u>ModelNet10/40数据集</u>是最常用的3D形状分类数据集。

‍

**三维形状分类方法：** 

#### <span data-type="text" style="color: var(--b3-font-color8);">（1）多视图方法（multi-view based methods）：</span>

将无结构点云投影为二维图像

‍

##### **Multi-view CNN**（MVCNN / 多视角卷积神经网络）：

用物体的三维数据从不同“视角”所得到的二维渲染图作为原始的训练数据，用经典的二维图像卷积网络进行训练。但最大池化只保留每个视图的最大元素，导致信息丢失。

（依靠二维图像分类网络来提取多视图特征，然后通过最大池聚合来获得一个紧凑的形状描述符。）

[论文阅读：Multi-view Convolutional Neural Networks for 3D Shape Recognition_upright orientation of 3d shapes with convolutiona-CSDN博客](https://blog.csdn.net/u010167269/article/details/51498927)

‍

##### **MHBH（Metropolis-Hastings算法）：**

是一个有效的<u>抽样方法</u>，是一种马尔可夫链蒙特卡罗（MCMC）方法，主要用于从复杂的概率分布中采样，尤其是当直接采样困难或不可行时。它在统计学、机器学习和贝叶斯推断等领域中非常常用。在三维形状分类中，结合了局部卷积特征，通过协调的双线性池化，生成紧凑的全局描述符。

‍

##### **View-GCN（view-based Graph Convolutional Neural Network，基于视图的图卷积神经网络）：**

用于在灵活的试图配置空间中识别基于多视图物体的图表示的3D形状。首先利用3D物体的2D多视图构建视图-图（view-graph）作为图节点，然后在视角图上设计一个图卷积神经网络分层地学习判别多视图物体的3D形状描述。

在第l层，视图-图Gl有Nl个节点，也就是视图。在视图-图上，我们通过局部图卷积和非局部信息传递来连续更新节点特征。然后，通过我们提出的选择性视图采样策略粗化图Gl，构建下一级视图-图Gl+1，以增加感受野，有利于语义特征学习。所有级别的特征被融合成一个全局形状描述符。与在一次集合操作中合并所有多视图特征的max-pooling相比，我们的view-GCN考虑到视图的关系，在分层粗化的视图-图上逐步合并多视图特征，并且所有级别的特征都被用于形状描述符中。

[论文分析：View-GCN View-based Graph Convolutional Network for 3D Shape Analysis_view-gcn: view-based graph convolutional network f-CSDN博客](https://blog.csdn.net/weixin_43499292/article/details/122238046)

‍

#### <span data-type="text" style="color: var(--b3-font-color8);">（2）体素方法（volu metric-based methods）：</span>

将点云转换为三维体素表示，即将点云体素化为三维网格，然后在体素表示上应用三维卷积神经网络（CNN）进行形状分类。

什么是体素化：[体素化(Voxelization)_体素化的参数-CSDN博客](https://blog.csdn.net/Xu_Haocan/article/details/78327234)

‍

##### **VoxNet（一种用于实时目标检测的3D卷积神经网络）** ：

VoxNet方法是==Volumetric Occupancy Grid representation==和 ==Supervised 3D Convolutional Neural Network (3D CNN)== 相结合的方法。

VoxNet将点云数据以 3D 体素网格 (Voxel Grid) 的形式表示，并在此基础上使用 3D 卷积网络进行特征学习。

VoxNet 的核心是将 <u>稀疏无序的点云</u> 转换为 <u>规则的三维体素网格</u>，然后使用 <u>3D 卷积网络</u> 进行端到端学习。

整个网络的输入是 <u>3D 体素网格</u>，输出是分类 / 检测 / 分割等任务的<u>预测值</u>。

[VoxNet(IEEE/RSJ 2015) - 知乎](https://zhuanlan.zhihu.com/p/65420219)

[【点云处理之论文狂读经典版3】——VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition-CSDN博客](https://blog.csdn.net/yuanmiyu6522/article/details/124582007?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522b70f0e0288345caa1b0e2fecb8be26b5%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=b70f0e0288345caa1b0e2fecb8be26b5&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-1-124582007-null-null.142^v102^pc_search_result_base2&utm_term=VoxNet&spm=1018.2226.3001.4187)

[点云网络（Point Clouds Network）—— VoxNet-CSDN博客](https://blog.csdn.net/qq_44648285/article/details/146044840?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522b70f0e0288345caa1b0e2fecb8be26b5%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=b70f0e0288345caa1b0e2fecb8be26b5&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-146044840-null-null.142^v102^pc_search_result_base2&utm_term=VoxNet&spm=1018.2226.3001.4187)

‍

##### **convolutional deep belief-based  3D ShapeNets(CDBN,卷积深度信念网络)** ：

深度信念网络 (DBN)  是一类强大的<u>概率模型</u>，通常用于对二维图像中像素和标签的联合概率分布进行建模。在这里，我们将模型从 2D 像素数据调整为 <u>3D 体素数据</u>，具有合理分辨率（例如 30 × 30 × 30）的 3D 体素体积将与高分辨率图像（165 × 165）具有相同的尺寸。此类图像上的全连接 DBN 会导致大量参数，使模型难以有效训练。因此，我们建议使用卷积来通过权重共享来减少模型参数。

[3D ShapeNets: A Deep Representation for Volumetric Shapes翻译-CSDN博客](https://blog.csdn.net/weixin_42037651/article/details/120325839)

‍

##### **OctNet(Octree Networks，基于八叉树的卷积神经网络)：**

是一种专门处理稀疏3D数据（如点云、体素网格）的<u>神经网络架构</u>，它结合了八叉树数据结构的高效<u>空间划分能力</u>和CNN的<u>层次特征提取能力</u>。

输入数据类型是<u>稀疏3D数据（点云、体素网格）</u>，预处理通常将输入数据转换为八叉树表示，每个节点存储（几何信息，例如是否包含表明；属性信息，例如颜色、法向量、TSDF值）。输入格式为显示八叉树 / 隐式编码。

输出数据根据任务目的不同而不同，若为分类，则输出类别概率；若为分割，则输出逐点/逐体素标签；若为检测，则输出边界框+类别

OctNet适用于<u>稀疏3D数据</u>的深度学习表示方法

利用输入数据的稀疏性，使用非平衡八叉树将空间按照层次分割，八叉树的叶子节点存储着pooled feature representation.

ps:下采样包含了池化，池化属于下采样，但是下采样不仅仅只有池化[下采样和池化的区别_池化和下采样-CSDN博客](https://blog.csdn.net/qq_31347869/article/details/90378030)

通过对点云进行分层分区，将场景表示为规则网格上的多个八叉树，能够高效分配存储，每个体素的特征由简单平均值进行编码。

[【点云处理之论文狂读经典版4】——OctNet: Learning Deep 3D Representations at High Resolutions-CSDN博客](https://blog.csdn.net/yuanmiyu6522/article/details/124644771)

‍

##### **PointGrid（点与网格相结合的 三维卷积 网络）：**

<u>在每个网络单元中包含恒定数量的点</u>，从而允许网络学习更高阶的局部逼近函数，可以更好地<u>表示局部几何形状细节</u>。

PointGrid 是一种用于处理 3D点云数据 的深度学习框架，通过将点云转换为 规则化的网格结构（Grid），结合 3D卷积神经网络（CNN） 进行特征提取。它旨在解决点云数据的稀疏性和无序性问题，同时利用传统CNN的高效局部特征学习能力。

输入数据：原始数据（点云数据，如.ply/.bin格式）

输入数据进行预处理：体素化（将点云嵌入到均匀的3D网格中，每个网格单元包含落入其内部的点的统计特征）；特征填充（若网格单元非空，存储点特征；若为空，填充零或默认值。)

输出：若任务为分类，则输出类别概率；若任务为分割，则输出逐点或逐体素的标签；若任务为检测，则输出边界框参数；

[PointGrid - 知乎](https://zhuanlan.zhihu.com/p/348096895)

[Paper reading：PointGrid： A Deep Network for 3D Shape Understanding_pointgrid: a deep network for 3d shape understandi-CSDN博客](https://blog.csdn.net/Felaim/article/details/82468379)

‍

##### **3DmFV（三维修正Fisher矢量）：**

一种性能优秀的三维点云<u>表示方法</u>。通过<u> Fisher向量</u>（Fisher vector本质上是用似然函数的梯度vector来表达一幅图像）<u>+网格化 </u>使CNN能高效处理3D数据，适用于 分类、分割等任务。输入是<u> 点云</u>，输出取决于<u>任务（如类别概率或检测框</u>），若任务是分类，则输出类别概率统计；若任务是分割，则输出逐点标签；若任务是检测，则输出边界框+类别。相比纯体素方法，它在 计算效率 和 特征保留 上更具优势

3DmFV网络分类架构：（1）将输入点云转换为3DmFV表示 （2）将转化后的架构输入CNN架构

Fisher向量：是一种基于统计的特征编码方法，通过概率模型的梯度统计将一组局部特征（如图像局部描述符、3D点云数据）编码成一个全局特征向量。

‍

‍

#### <span data-type="text" style="color: var(--b3-font-color8);">（3）点云方法（point-based methods）：</span>

直接在原始点云上工作，无需体素化或投影。点云方法不会引入显性信息损失。

##### <span data-type="text" style="color: var(--b3-font-color6);">①Pointwise MLP Methods（逐点MLP方法）</span>

###### **PointNet：**

该<u>模型</u>可直接对<u>点云</u>进行处理，对输入点云中的每一个点，学习其对应的<u>空间编码</u>，之后再利用所有点的特征得到一个<u>全局的点云特征</u>。其局部特征提取能力较差，即每个点的特征是独立学习的，因此无法捕捉点与点之间的局部结构信息，这使得它很难对复杂场景进行分析。

[一文搞懂PointNet全家桶——强势的点云处理神经网络-CSDN博客](https://blog.csdn.net/Yong_Qi2015/article/details/128509981)

‍

###### **DeepSets：**

这是一种神经网络架构，可以处理大小可变的集合输入，提出的基于对称函数的架构可以确保网络的输出不依赖于集合中元素的顺序。

网络架构包含两部分：

①处理单个元素的变换函数：变换函数负责将集合中的每个元素映射到一个新的空间

②对称函数：对称函数对这些映射后的元素（经过变化函数映射后的元素）进行聚合，得到最终的输出。

[Deep Sets阅读笔记及总结（论文总结、代码复现、论文翻译） - 知乎](https://zhuanlan.zhihu.com/p/662789491)

‍

###### **PointNet++：**

是PointNet的改进版本，其核心是提出了多层次特征提取结构，有效提取局部特征提取，和全局特征。可用于捕捉每个点邻域内的精细几何结构。

PointNet++分层结构的核心由三部分组成：<u>采样层</u>、<u>分组层</u>和<u>基于PointNet的学习层</u>。通过堆叠多个集合抽象层，PointNet++能够逐层学习局部几何结构的特征，并进行抽象。

[一文搞懂PointNet全家桶——强势的点云处理神经网络-CSDN博客](https://blog.csdn.net/Yong_Qi2015/article/details/128509981)

‍

###### **Mo-Net（Morphable Point Network**） **：**

Mo-Net 是一种用于<u> 3D点云生成与变形 </u>的深度学习模型，它结合了<u> 归一化流（Normalizing Flows）</u> 和<u> 点云处理网络</u>，能够从隐空间生成多样化的3D点云，或对现有点云进行可控变形。

核心思想：

1）点云生成：从随机噪声生成合理的3D点云（如椅子、飞机等）。

2）点云变形：对输入点云进行平滑、结构保持的形变（如调整椅子腿长度）。

3）特征学习：学习点云的隐式表示（Latent Space）。

关键技术;

1）归一化流（Normalizing Flows）：通过一系列可逆变换（MLP实现），将简单分布（如高斯分布）映射到复杂点云分布。

2）基于点的MLP（Point-wise MLP）：对每个点独立应用多层感知机，保持点云的无序性。

3）层次化变形：逐步细化点云形状，从粗到细生成或变形。

‍

###### **Point Attention Transformers（PATs）:**

是一种<u>基于 Transformer 架构</u>的神经网络模型，专门用于处理点云数据，尤其是在 3D 计算机视觉任务中，比如物体检测、语义分割等。它是为了处理传统 Transformer 在处理稀疏、不规则数据（如点云）时存在的不足而设计的。

输入：

  PATs 的输入通常是点云数据。在 3D 空间中，点云是一种通过激光扫描、立体视觉或其他传感器收集的离散的 3D 点集，每个点包含空间坐标（x, y, z）以及可能的额外信息（如颜色、强度、法线等）。

* **点云数据**：每个点在三维空间中具有坐标和其他可能的特征信息。
* **额外的特征**：如点的属性（比如强度、颜色、纹理等），可以作为额外的输入。

输出：

PATs 的输出通常依赖于具体的任务：

* **物体检测**：输出物体的边界框、类别标签等。
* **语义分割**：输出每个点的类别标签。
* **三维重建**：输出预测的三维物体表面。

作用：

在传统的卷积神经网络（CNN）处理图像时，模型通过网格结构的像素来学习局部特征，而点云数据是稀疏和无序的，直接应用 CNN 在点云上效果并不好。PATs 通过采用 **自注意力机制（Self-Attention）** ，使得模型能够在处理点云时关注点之间的关系，不受点云的排列顺序和稀疏性影响。其主要作用如下：

* **解决点云数据稀疏和无序性问题**：通过自注意力机制，PATs 能够在点与点之间建立长距离的依赖关系，而不是像 CNN 那样只能处理局部邻域的关系。
* **提高点云特征学习能力**：能够更好地捕捉点云中点之间的空间关系和全局上下文信息。
* **高效地处理大规模点云数据**：由于 Transformer 的结构可以并行计算，PATs 可以高效地处理大规模点云数据。

‍

###### **Group Shuffle Attention (GSA)：**

是一种改进的注意力机制，旨在提升 Transformer 模型的效率和表示能力，特别是在处理大规模数据和多组特征时。它通过将注意力操作<u>划分为多个小组</u>并对<u>小组间进行“洗牌”</u>来优化计算流程，进而提高性能。

输入：

GSA 的输入通常是特征序列或特征图，这些特征可能来自图像、文本、音频或其他结构化数据，具体取决于任务。

* **输入特征**：对于 NLP 任务，输入通常是词嵌入序列；对于视觉任务，则可能是图像的特征图。
* **特征维度**：这些输入数据的维度通常包括序列长度和特征维度（例如，文本中的词嵌入维度或图像中的卷积特征维度）。

输出：

GSA 的输出通常是一个经过加权的特征序列，输出的特征可以用于下游任务，如分类、回归、生成等。

* **输出特征**：经过 GSA 处理后的特征表示，可以用于进行进一步的决策，比如分类标签、回归值或生成的图像等。

作用：

Group Shuffle Attention 的核心思想是<u> 将输入特征划分为多个小组，并通过“洗牌”操作重新组合这些小组的注意力信息。</u>这种做法的作用如下：

* **提高计算效率**：传统的自注意力机制计算复杂度为 $O(n^2)$（n为序列长度），当处理大规模数据时会变得非常低效。GSA 通过划分组和在组内进行计算，降低了每个组内的计算量，从而优化了性能。
* **增强模型表达能力**：GSA 通过“洗牌”操作（即重组注意力的分配方式），使得模型能够在多个组之间建立更加复杂的依赖关系。这有助于捕捉不同特征之间的多样性和丰富性。
* **降低注意力计算的复杂度**：通过在多个组内并行处理注意力机制，GSA 可以在保证模型表现的前提下减少计算资源的消耗，并且避免了全局注意力计算的昂贵开销。
* **提高特征融合的多样性**：洗牌操作使得每个小组内部的特征被重新组合，能够产生更丰富的上下文信息，增强模型的多任务处理能力。

‍

###### **Gumbel Subset Sampling (GSS)：**

是一种基于 Gumbel 分布的<u>采样方法</u>，常用于强化学习、生成建模和其他需要从离散空间中选择子集的任务。其主要作用是<u>通过模拟从离散空间中选择子集的过程，来处理复杂的离散优化问题</u>。GSS 被用于高效地<u>选择、采样或推断</u>离散动作或状态子集，特别是在强化学习和生成模型中。

输入：

Gumbel Subset Sampling 的输入通常是离散选择问题中相关的概率分布或评分函数，具体来说：

* **概率分布或评分函数**：假设你需要从一组候选项中选择一个子集（例如，强化学习中的动作选择，或者在生成建模中的某些离散事件选择）。GSS 输入的是这些候选项的评分（或概率）分布。
* **离散候选项**：对于每个候选项（如动作、特征等），你将给定其一个评分或概率值。GSS 会基于这些值进行采样。

输出：

Gumbel Subset Sampling 的输出是根据输入的评分或概率分布，从离散候选项中选择的一个子集。

* **子集选择**：输出的是一个在给定的概率或评分分布下<u>选择的最优子集</u>。例如，在强化学习中，输出可能是最优的动作；在生成建模中，输出可能是最合适的特征选择或离散变量的配置。

作用：

Gumbel Subset Sampling 的核心目的是通过引入 Gumbel 分布来对离散空间进行<u>高效的采样</u>。其主要作用如下：

* **处理离散优化问题**：在许多任务中，尤其是离散决策问题（例如，从多个候选动作中选择一个动作），传统的优化方法难以直接应用，因为离散空间中没有明显的梯度信息。GSS 通过使用 Gumbel 分布的采样方法，能够模拟从离散空间中选择最优子集的过程。
* **平滑离散选择过程**：Gumbel 分布在采样时会引入随机性，模拟“选择”离散候选项的过程。这使得通过 Gumbel 采样可以在离散空间中高效地探索，并且由于其基于概率的性质，可以在训练过程中进行梯度优化（例如，通过策略梯度方法）。
* **增强模型的探索与利用**：在强化学习中，GSS 可以帮助智能体在训练时平衡探索和利用。它通过随机选择离散动作或状态，促进探索，而同时在选择时考虑评分，从而有效地利用当前获得的知识。
* **提升离散选择任务的性能**：Gumbel Subset Sampling 能够解决传统离散采样方法（如贪心策略）的一些不足之处，尤其是在面对复杂的多维离散空间时，能够提高性能并降低计算复杂度。

‍

###### **Adaptive Feature Adjustment (AFA)：**

是一种用于提升点云数据特征学习的技术，特别是在处理点云数据中的<u>局部上下文</u>时。它通常与其他方法（如 PointWeb）结合使用，以<u>增强每个点的特征表示</u>。

输入：

AFA 的输入通常是从点云中提取的特征，这些特征可以是从点云数据中获取的初始点特征，也可能是在某些网络层（如 PointNet 或 PointNet++）后生成的特征。

* **点云特征**：这些是点云中每个点的初步特征，可能包括空间位置、法线信息、颜色或其他附加特征。
* **局部上下文信息**：AFA 通过对每个点的邻域信息进行调整来处理这些输入特征。

输出：

AFA 输出的是经过自适应调整后的点特征，具体来说：

* **增强的特征表示**：每个点的特征在考虑局部邻域信息的基础上进行了调整，这使得每个点的特征更加适应于当前任务，如点云的分类或分割。

作用：

AFA 的主要作用是 <u>通过自适应地调整局部特征来增强点的表示能力</u>。这对于捕捉点云中<u>点与点之间的复杂关系</u>，特别是<u>局部结构信息</u>，具有重要意义。

* **增强局部信息**：AFA 能够根据点的邻域特征自适应地调整每个点的表示，优化点特征的表达，使得点云中相邻点的特征能够更好地进行整合。
* **提升特征学习**：通过自适应调整，AFA 能够捕捉到点云数据中的更丰富的上下文信息，提升模型对点云中局部结构的理解，从而提高模型的整体性能。
* **改善分割与分类任务的效果**：在点云分割和分类等任务中，AFA 可以通过改进点特征，帮助模型更准确地区分不同类别的点或区域。

‍

‍

###### **Structural Relational Net work (SRN)：**

是一种用于3D点云处理的深度学习网络，专门设计来学习点云中<u>不同局部结构之间的关系</u>。SRN 主要用于<u>点云的特征学习</u>，尤其是通过学习点云中<u>不同局部区域之间的结构关系</u>，从而提高点云分类、分割等任务的效果。

输入：

SRN 的输入通常是点云数据的<u>特征</u>，具体包括：

* **点云数据**：包含3D空间中每个点的坐标及其他可能的附加特征（如颜色、法线等）。
* **局部结构信息**：这些特征经过一定的处理后，会通过网络学习点与点之间的关系，尤其是点云中局部区域的结构关系。

输出：

SRN 的输出是经过学习后的<u>特征表示</u>，具体包括：

* **结构化特征**：通过学习点云中不同局部区域之间的关系，SRN 输出增强的点特征，这些特征包含了点云中不同局部区域之间的几何和结构信息。
* **分类或分割结果**：在点云的分类或分割任务中，SRN 的输出通常是每个点的类别标签，或者是点云的全局或局部特征表示。

作用：

主要作用是 <u>学习点云中局部结构之间的关系</u>，通过这种学习来提升点云数据的特征表示能力。具体来说，SRN 起到以下几个作用：

* **学习局部结构关系**：SRN 通过在不同局部结构之间建立关系，捕捉到点云中各点和其邻域之间的几何和语义关联。这使得它能够在处理点云时，考虑到点之间的空间关系，从而获得更加准确的特征表示。
* **提升特征表达能力**：通过对局部结构关系的建模，SRN 能够增强点的特征表示，使得这些特征能更好地适应点云分类、分割等任务。
* **适应复杂几何结构**：SRN 特别适合处理复杂的几何结构，尤其是当点云中的点分布不均匀，且局部结构复杂时，SRN 能够通过学习这些复杂结构的关系，提升模型的鲁棒性。

###### Adaptive Sampling (AS)：

是一种用于点云数据处理的技术，旨在通过<u>动态地选择和调整采样点</u>来优化学习过程，尤其是在处理点云数据时。它有助于提高点云学习模型的效率和效果，特别是在<u>稀疏、复杂的点云数据</u>中。

输入：

Adaptive Sampling 的输入通常是<u>点云数据</u>，具体来说：

* **点云数据**：这包括点云中的每个点的空间坐标及其他可能的附加特征（如颜色、法线、强度等）。
* **采样策略**：通常会使用一些初始的采样方法，如最远点采样（FPS）来选取一部分点作为初步的采样点。AS 的关键在于根据点云的局部特征动态调整这些采样点。

输出：

AS 的输出通常是 **优化后的采样点集**，这些采样点能够代表点云中的重要信息，通常用于进一步的处理，如特征学习、分类或分割等任务。

* **采样点集**：AS 输出一个精心挑选的点集，这些点在空间中可能分布更为均匀且代表性更强。
* **改进的特征**：通过自适应采样后，输出的点云特征通常能够捕捉到更多的局部结构信息，提升模型对点云的理解。

作用：

Adaptive Sampling (AS) 的主要作用是通过 **自适应地选择代表性强的点进行采样**，从而提高点云处理的效率和效果。其具体作用包括：

* **优化采样效率**：AS 根据点云中不同区域的密度和重要性动态调整采样点，避免在不重要的区域浪费计算资源，同时聚焦在具有更多信息的区域。
* **提高局部特征学习能力**：自适应采样能够根据点云中局部区域的几何特征进行点的选择，确保学习到的特征能够充分代表点云中不同区域的几何结构，从而提高分类或分割任务的准确性。
* **提升稀疏数据的处理能力**：在点云数据稀疏的情况下，AS 能够帮助模型在缺失信息的区域做出更智能的选择，从而减少因数据不均匀而导致的性能下降。
* **减少计算开销**：通过只选择最具代表性的点，AS 可以显著减少需要处理的数据量，进而加速计算过程，特别是在处理大规模点云数据时。

###### Furthest Point Sampling (FPS)：

是一种常用于点云数据处理的<u>采样方法</u>，它通过选择点云中<u>最远的点来进行采样</u>，从而实现对点云的<u>均匀采样</u>。FPS 广泛应用于点云的<u>下采样</u>、<u>特征学习</u>和<u>模型训练</u>中，尤其在处理大规模点云数据时非常有效。

输入：

Furthest Point Sampling 的输入通常是点云数据，具体来说：

* **点云数据**：输入是一组 3D 点云数据，每个点都有其坐标（通常是 $x$、$y$、$z$ 坐标），并且可能包含额外的特征，如颜色、法线等。
* **采样点数**：FPS 还需要指定要从点云中采样的点数，即希望从原始点云中选择多少个代表性点。

输出：

Furthest Point Sampling 的输出是：

* **选中的采样点集**：FPS 输出的是通过迭代选择最远点获得的一个点子集。这个子集包含了从原始点云中选择的若干个点，这些点在空间中尽可能地均匀分布。
* **代表性点集**：这些采样点能够较好地代表原始点云的几何形状，尤其是在点云分布较为稀疏或不规则的情况下，FPS 能够确保所选的点较为均匀且分布合理。

作用：

Furthest Point Sampling 的主要作用是 <u>均匀地选择点云中的代表性点</u>，它能够在保持点云空间信息的同时，减少计算复杂度。具体来说，FPS 起到以下作用：

* **均匀采样**：FPS 会选择离当前已选点最远的点，这样可以确保选中的点在点云中均匀分布，避免集中在某个区域，从而获得更全面的空间覆盖。这对于点云的下采样或特征学习非常重要。
* **减少计算量**：通过从点云中选择较少但具有代表性的点，FPS 可以大幅度减少数据的数量，从而降低计算开销，尤其是在需要处理大规模点云数据时，FPS 是一种高效的采样方法。
* **改进局部和全局特征学习**：FPS 通过选择最远的点，帮助模型在训练过程中关注到空间上分布较广的点，这使得模型能够学习到点云中更丰富的局部和全局结构信息，尤其是在点云分类、分割等任务中，能够提升学习效果。
* **提高稀疏数据的处理效果**：在处理稀疏的点云数据时，FPS 可以帮助选择更多位于空白区域或边缘的点，确保这些区域的信息得到足够的表达，避免模型忽略这些稀疏区域。

‍

###### local-non-local(L-NL)：

是一种结合局部（local）和非局部（non-local）信息的特征学习模块，旨在<u>增强点云数据的上下文理解能力</u>，尤其是在处理点云中的<u>长距离依赖关系</u>时。L-NL 通过<u>将局部信息和全局信息有效结合</u>，增强模型的特征学习能力，特别是在捕捉点云中<u>局部和全局上下文的依赖关系</u>方面。

输入：

L-NL 的输入通常是点云数据的<u>特征表示</u>，具体来说：

* **点云特征**：输入是经过初步特征提取的点云数据特征，通常包含点的坐标信息及其在不同任务中提取的局部特征（例如颜色、法线、强度等）。
* **邻域信息**：L-NL 模块需要使用点云中每个点的局部邻域信息，这些邻域信息可以通过常见的采样方法（如最远点采样 FPS 或体素网格）来获得。

输出：

L-NL 模块的输出是<u>经过增强的点特征</u>，具体包括：

* **增强的点特征**：L-NL 输出的是经过局部和非局部信息融合后的点特征，这些特征能够更好地反映点云中各个点之间的复杂依赖关系。
* **全局和局部上下文信息**：通过结合局部和非局部特征，输出的特征包含了从局部邻域到全局范围的信息，增强了对点云的整体理解能力。

作用：

L-NL 的主要作用是 <u>通过结合局部和非局部信息来增强点特征的表达能力</u>，其作用可以具体总结为以下几点：

* **局部和非局部信息结合**：L-NL 模块结合了局部信息（即点的邻域）和非局部信息（即点与点之间长距离的关系），使得模型不仅能够捕捉点云中的细节，还能够考虑全局上下文信息。通过这种结合，L-NL 能更好地理解点之间的长距离依赖，尤其是在点云分割和分类任务中，能够提升模型性能。
* **捕捉长距离依赖**：L-NL 模块通过非局部连接，帮助模型捕捉到点云中各个点之间的远距离依赖关系，这对于那些需要全局上下文的任务（如物体检测、语义分割等）尤为重要。
* **提升特征学习的表达能力**：通过同时关注局部邻域的细节和全局信息，L-NL 模块提升了点特征的表达能力，尤其是在处理包含复杂几何结构和不规则分布的点云数据时，能够获得更有代表性的特征。
* **改进长程依赖问题**：在传统的点云处理方法中，模型往往更擅长捕捉局部信息，而忽略了长距离的依赖关系。L-NL 模块通过增强点之间的长程依赖关系，帮助解决这一问题。

‍

##### <span data-type="text" style="color: var(--b3-font-color6);">②Convolution-Based Methods（基于卷积的方法）</span>

###### a）3D连续卷积方法：

<span data-type="text" style="background-color: var(--b3-font-background1);">这些方法通过在 </span>**连续空间**<span data-type="text" style="background-color: var(--b3-font-background1);"> 中定义卷积核，利用点与点之间的几何关系来实现卷积。</span>

**RS-Conv (RS-CNN)**

* **输入**：输入是点云数据的局部子集，选择某个点周围的邻域点作为输入。
* **输出**：输出是经过卷积操作后获得的点特征。这些特征是从低级关系（如欧几里得距离和相对位置）映射到高层次点关系的结果。
* **作用**：RS-Conv 是 RS-CNN 的核心层，用于通过卷积学习点云中<u>局部点之间的关系</u>。它通过MLP（多层感知机）来学习<u>低级和高级的点之间的几何和语义关系</u>。

**Kernel Point Convolution (KPConv)**

* **输入**：输入是点云数据，每个点会有一组与之相关的邻域点。
* **输出**：输出是通过卷积操作学习到的点特征，这些特征包含了局部点之间的几何关系和高层次的语义信息。
* **作用**：KPConv 提出了刚性和可变形卷积操作符，使用一组可学习的核点来进行卷积。它通过学习点云中点之间的关系来<u>增强点特征的表示</u>，尤其适用于3D点云的<u>几何形状建模</u>。

**ConvPoint**

* **输入**：输入是点云数据，每个点有其空间位置及附加特征。
* **输出**：输出是通过卷积核对点云特征进行处理后生成的新的特征表示。
* **作用**：ConvPoint 将卷积核分为<u>空间部分</u>和<u>特征部分</u>。空间部分的位置通过随机从单位球内选择，而特征部分通过MLP来学习权重函数。它主要用于将局部点云特征与空间信息结合，通过学习空间和特征的关系来<u>增强点云的表示能力</u>。

 **PointConv**

* **输入**：输入是点云数据以及与之相关的特征，卷积核的权重函数和密度函数通过MLP学习。
* **输出**：输出是通过卷积得到的<u>点云特征</u>，包含了点之间的关系和几何信息。
* **作用**：PointConv 通过蒙特卡洛估计进行卷积，并基于重要性采样定义卷积核。它通过学习权重和密度函数，能够<u>处理点云中的不规则性</u>，并通过简化的矩阵乘法和2D卷积来提高计算效率。

**MCCNN**

* **输入**：输入是点云数据，通过MLP实现卷积核的学习。
* **输出**：输出是通过卷积运算得到的点特征，基于点云的采样密度进行调整。
* **作用**：MCCNN 通过蒙特卡洛估计来进行卷积，它依赖于样本的密度函数，并使用泊松盘采样来构建点云层次结构，处理不同采样密度的点云数据。

**SpiderCNN (SpiderConv)**

* **输入**：输入是点云数据和邻近点的几何信息，特别是k个最近邻点的信息。
* **输出**：输出是通过卷积操作后的点云特征，包含了点之间的几何关系。
* **作用**：SpiderConv 将卷积定义为阶跃函数与泰勒展开的乘积。它通过编码局部测地距离来捕捉粗糙几何形状，通过泰勒展开来捕捉局部几何变化，从而有效地进行卷积操作。

**PCNN (Point Cloud Neural Network)**

* **输入**：输入是点云数据，通过径向基函数（RBF）来提取点云特征。
* **输出**：输出是通过径向基函数进行卷积操作后得到的点特征表示。
* **作用**：PCNN 是基于径向基函数的点云卷积网络，主要用于提取点云中局部区域的几何特征，能够捕捉复杂的空间结构和特征。

**3D Spherical CNN**

* **输入**：输入是多值球面函数，通常用于表示3D形状的旋转不变性。
* **输出**：输出是球面卷积操作后得到的点云特征，具备旋转等变性。
* **作用**：3D Spherical CNN 旨在学习旋转等变的表示，通过使用球面谐波参数化谱，结合锚点来获取局部卷积滤波器，从而实现旋转不变性。

**Tensor Field Networks**

* **输入**：输入是点云数据，采用球面谐波的形式进行输入。
* **输出**：输出是通过球面谐波和可学习的径向函数计算得到的点特征。
* **作用**：Tensor Field Networks 定义了点云卷积操作作为可学习的径向函数与球面谐波的乘积，具有对3D旋转、平移和排列的局部等变性，适用于处理旋转不变的任务。

 **Flex-Convolution**

* **输入**：输入是点云数据以及邻近点的特征。
* **输出**：输出是通过卷积得到的点云特征。
* **作用**：Flex-Convolution 将卷积核的权重定义为k个最近邻的标准标量积，通过CUDA加速实现，旨在提高计算速度，适用于计算资源有限的环境。

###### b）3D离散卷积方法：

<span data-type="text" style="background-color: var(--b3-font-background1);">离散卷积方法则在规则的 </span>**网格结构**<span data-type="text" style="background-color: var(--b3-font-background1);"> 上定义卷积核，其中卷积核权重与中心点的偏移量相关。</span>

**Hua et al.'s** **<u>3D Grid Convolution</u>**

* **输入**：输入是非均匀的3D点云，经过转换为均匀网格。
* **输出**：输出是经过卷积操作后得到的点云特征，所有点在同一网格中的特征被计算并加权。
* **作用**：通过将点云数据转换为均匀网格，定义卷积核，将相同权重分配给同一网格中的点，适用于处理点云的不规则性，尤其是转换为网格结构后。

 **Lei et al.'s** **<u>Spherical Convolution</u>**

* **输入**：输入是3D球形邻域的数据，卷积核在球形邻域中分布。
* **输出**：输出是经过加权激活后的点云特征。
* **作用**：Lei 等人定义了一个球形卷积核，将3D邻域划分为多个体积单元，利用可学习的权重矩阵进行卷积。它通过加权激活值的均值进行非线性激活，用于处理球形区域的数据。

**GeoConv**

* **输入**：输入是点云数据，以及通过六个基准建立的几何关系。
* **输出**：输出是基于六个基准计算后的点云特征。
* **作用**：GeoConv 明确建模了点与其邻域点之间的几何关系，使用与方向相关的可学习矩阵来加权这些特征，增强点云的几何信息处理能力。

 **PointCNN**

* **输入**：输入是点云数据，通过x-conv变换（MLP实现）将点数据转换为潜在的标准化顺序。
* **输出**：输出是通过卷积操作得到的点云特征。
* **作用**：PointCNN 通过x-conv变换将点转换为标准化顺序，然后应用常规卷积操作，从而学习点之间的关系，适用于处理点云中的不规则数据。

**InterpConv**

* **输入**：输入是点云数据，卷积核与离散的点特征坐标有关。
* **输出**：输出是通过插值卷积操作得到的点云特征。
* **作用**：InterpConv 通过将点特征插值到邻近的卷积核权重坐标，度量输入点云与核坐标之间的几何关系，增强卷积操作的表达能力。

**RIConv**

* **输入**：输入是具有低级旋转不变几何特征的点云数据。
* **输出**：输出是旋转不变的点云特征。
* **作用**：RIConv 通过简单的分箱方法将卷积转化为1D，旨在处理旋转不变的几何数据。

**A-CNN**

* **输入**：输入是点云数据，特别是查询点的邻居点数组。
* **输出**：输出是通过环形卷积操作得到的点云特征。
* **作用**：A-CNN 通过定义环形卷积核，学习局部邻域内的点特征关系，能够处理复杂的局部几何结构。

**ReLPV (Rectified Local Phase Volume)**

* **输入**：输入是点云数据，通过3D短时傅里叶变换提取相位。
* **输出**：输出是通过相位提取得到的点云特征。
* **作用**：ReLPV 通过3D短时傅里叶变换提取局部相位特征，减少了计算和内存开销，适用于低资源环境下的高效点云处理。

**SFCNN**

* **输入**：输入是点云数据，通过投影到规则的二十面体格栅上。
* **输出**：输出是通过卷积-最大池化-卷积结构得到的点云特征。
* **作用**：SFCNN 将点云数据投影到二十面体格栅上，通过卷积操作来处理球面坐标，具有较强的旋转和扰动抗性。

##### <span data-type="text" style="color: var(--b3-font-color6);">③Graph-Based Methods（基于图的方法）</span>

基于图的网络将点云中的每个点视为图的一个顶点，并基于每个点的邻居生成图的有向边。然后在空间域或谱域中进行特征学习。该方法分为：空间域中基于图的方法；谱域中基于图的方法

###### <span data-type="text" style="color: var(--b3-font-color4);">a）空间域中基于图的方法</span>

**Edge Conditioned Convolution (ECC)**

* **输入**：点云数据，每个点与其邻居的空间坐标、几何属性以及激光强度或颜色等特征。
* **输出**：通过卷积操作得到的点云特征，包含<u>每个点与邻居之间的关系</u>。
* **作用**：ECC 使用MLP（多层感知机）生成卷积滤波器，并通过最大池化聚合邻域信息。该方法通过边条件卷积在空间邻域上进行操作，适用于图的粗化和点云的特征学习。

**DGCNN (Dynamic Graph CNN)**

* **输入**：点云数据，特征空间中的图以及该图的动态更新。
* **输出**：通过EdgeConv层动态更新后的点云特征，聚合邻居点的信息。
* **作用**：DGCNN 在特征空间中构建图，并在每一层后动态更新图结构。通过EdgeConv（边卷积），每个点的邻居特征通过MLP聚合，从而实现点云特征学习。该方法擅长捕捉点之间的几何关系。

 **LDGCNN (Lightweight DGCNN)**

* **输入**：点云数据，DGCNN中各层的层次特征。
* **输出**：简化的点云特征，减少模型大小和计算量。
* **作用**：LDGCNN 去除了DGCNN中的转换网络，并将不同层的层次特征连接起来，以提高性能并减少模型大小。它保留了DGCNN的核心思想，适用于资源有限的环境。

**FoldingNet**

* **输入**：点云数据，向量化的局部协方差矩阵以及点坐标。
* **输出**：通过卷积和自编码器网络得到的点云特征，学习到的形状和点特征。
* **作用**：FoldingNet 是一个端到端的无监督自编码器网络。它将向量化的局部协方差矩阵与点坐标连接起来，通过自编码器网络进行点云特征的学习，适用于点云重建。

**Hassani and Haley’s Multi-Task Autoencoder**

* **输入**：点云数据，多尺度图结构。
* **输出**：通过多任务学习得到的点和形状特征。
* **作用**：该方法通过无监督的多任务自编码器来学习点和形状特征，结合聚类、自监督分类和重建任务，通过多任务损失共同训练。通过多尺度图来捕捉不同级别的特征。

 **Dynamic Points Agglomeration Module (DPAM)**

* **输入**：点云数据，点的特征矩阵。
* **输出**：聚合后的点特征，简化的点聚合过程。
* **作用**：DPAM 是基于图卷积的方法，用于简化点聚合过程（采样、分组、池化），通过聚合矩阵和点特征矩阵的乘法实现点的聚合。它基于PointNet架构，动态地利用点之间的关系，在语义空间中进行点的聚合。

**KCNet**

* **输入**：点云数据，局部结构的几何特征。
* **输出**：通过核相关学习得到的点特征。
* **作用**：KCNet 通过定义一组可学习的点作为核，来表征局部结构的几何类型。然后计算核与给定点邻域之间的亲和力，从而学习点云中的局部几何特征。

**G3D**

* **输入**：点云数据，邻接矩阵。
* **输出**：通过卷积和池化操作得到的点云特征。
* **作用**：G3D 将卷积定义为邻接矩阵的多项式变体，并通过拉普拉斯矩阵与顶点矩阵的乘法实现池化。它用于捕捉点云中点与点之间的关系，增强几何特征学习。

**ClusterNet**

* **输入**：点云数据，k个最近邻的点。
* **输出**：旋转不变的点云特征，通过EdgeConv块学习得到的特征。
* **作用**：ClusterNet 通过旋转不变模块从每个点的k个最近邻提取旋转不变的特征，并使用无监督的凝聚层次聚类方法构建点云的层次结构。该方法适用于处理具有旋转不变性的点云数据。

**Grid-GCN**

* **输入**：点云数据，通过体积方法和点云方法结合的数据结构。
* **输出**：通过计算提高计算效率的点云特征。
* **作用**：Grid-GCN 将体积方法与点云方法相结合，以提高计算效率。通过该方法，计算效率比其他模型平均提高了五倍，适用于大规模点云数据处理。

###### <span data-type="text" style="color: var(--b3-font-color4);">b）谱域中基于图的方法</span>

**RGCNN**

* **输入**：点云数据，通过图构建每个点与其他点的连接。
* **输出**：通过谱过滤得到的点云特征。
* **作用**：RGCNN 构建一个图，通过将每个点与点云中的所有其他点连接，并在每层更新图的拉普拉斯矩阵。它通过图信号平滑性先验来增强相邻顶点的特征相似性，适用于点云特征的学习。

**AGCN (Attention Graph Convolutional Network)**

* **输入**：点云数据，图的邻接矩阵。
* **输出**：基于图学习的点云特征，结合可学习距离度量。
* **作用**：AGCN 利用可学习的距离度量来参数化图上两个顶点之间的相似性，采用高斯核和学习到的距离对图的邻接矩阵进行归一化，适合用于多种不同图拓扑结构的数据。

**HGNN (Hypergraph Neural Network)**

* **输入**：点云数据，通过超图构建图。
* **输出**：通过超边卷积得到的点云特征。
* **作用**：HGNN 通过在超图上应用谱卷积来构建超边卷积层，能够处理更复杂的图结构，尤其是在点之间存在高阶关系时。

**LocalSpecGCN**

* **输入**：点云数据，基于k个最近邻构建的局部图。
* **输出**：局部图谱卷积得到的点云特征。
* **作用**：LocalSpecGCN 是一个端到端的谱卷积网络，专门在局部图上工作，使用局部图谱卷积来学习点云中的局部结构，避免了离线计算图的拉普拉斯矩阵和粗化层次结构。

**PointGCN**

* **输入**：点云数据，k个最近邻点构建的图。
* **输出**：通过谱卷积得到的点云特征，结合高斯核加权的边。
* **作用**：PointGCN 通过k个最近邻构建图，并使用高斯核对每条边进行加权。在图谱域中，卷积滤波器通过切比雪夫多项式定义，适用于捕捉点云的全局和局部特征。

**3DTI-Net**

* **输入**：点云数据，通过k个最近邻构建的图。
* **输出**：几何变换不变的点云特征。
* **作用**：3DTI-Net 在谱域中对k个最近邻图应用卷积，通过学习相对欧几里得距离和方向距离来实现几何变换的不变性，适用于几何变换问题的点云特征学习。

‍

##### ④Hierarchical Data Structure-Based Methods（基于层次数据结构的方法）

基于层次数据结构（如八叉树和K-d树）构建的网络中，各方法利用<u>树形结构</u>从<u>叶节点到根节点逐步学习点云数据的特征</u>。

**八叉树引导卷积神经网络（Octree Guided CNN）**

* **输入**：点云数据，每个点都有坐标、强度、颜色等特征。
* **输出**：通过每一层的卷积操作，最终得到的点云特征表示，适用于点云分类或分割任务。
* **作用**：Octree Guided CNN 利用八叉树结构来高效地处理3D点云数据。每一层的卷积操作都通过球形卷积核来实现，每一层对应八叉树的一个层级。在每一层，当前层的神经元值是通过计算前一层所有相关子节点的均值来得到的。该方法能够有效利用树结构进行层次化特征学习，并能够减少计算量。

---

**K-d树网络（K-d Net）**

* **输入**：点云数据，每个点的坐标和特征信息。
* **输出**：通过卷积和层次结构学习得到的点云特征，最终用于预测分类分数。
* **作用**：K-d Net 使用多棵K-d树进行构建，并在每次迭代中采用不同的分割方向，基于自下而上的方法学习点云特征。每个非叶节点的特征是通过MLP（多层感知机）从其子节点的特征计算得到。最后，根节点的特征（描述整个点云）被传递到全连接层进行分类预测。K-d Net 的一个特点是它在每层共享参数，参数的类型根据节点的分割方式来决定。

---

**3DContextNet**

* **输入**：点云数据，标准的平衡K-d树结构。
* **输出**：通过学习和聚合得到的点云特征，最终用于分类任务。
* **作用**：3DContextNet 使用平衡K-d树来实现点云特征的学习和聚合。每一层的点特征通过MLP来学习，利用局部线索（点云中局部区域点的依赖关系）和全局上下文线索（点与其他点的关系）。每个非叶节点的特征通过MLP从其子节点计算得到，并通过最大池化进行特征聚合。在分类任务中，该过程会重复，直到得到根节点的特征。3DContextNet 将层次结构与特征聚合结合，适用于大规模点云数据的处理。

---

**SO-Net（Self-Organizing Network）**

* **输入**：点云数据，经过点到节点的k最近邻搜索得到的点和节点坐标。
* **输出**：通过SOM模型学习得到的点云特征，最终用于分类或其他任务。
* **作用**：SO-Net 使用一种修改过的自组织映射（SOM）模型来建模点云的空间分布。网络通过执行点到节点的k最近邻搜索来构建层次结构。在SOM中，每个点的特征通过一系列全连接层从标准化的点到节点坐标中进行学习。通过通道最大池化提取每个节点的特征，并结合PointNet类似的方法，从节点特征中学习最终的点云特征。与PointNet++相比，SO-Net 提供了更高效的层次结构，同时充分探索了点云的空间分布。

##### ⑤Other Methods（其他方法）

涉及到了**3D点云表示学习**和**特征增强**等技术

**RBFNet**

* **输入**：点云数据，特征为点云中每个点的位置以及其他可能的属性（如颜色、强度等）。
* **输出**：通过径向基函数（RBF）核进行学习后得到的点云特征表示。
* **作用**：RBFNet 通过聚合稀疏分布的径向基函数（RBF）核的特征来明确建模点云中点的空间分布。RBF核的位置和大小是可学习的，旨在提升点云的几何表示和语义表达能力，特别是在稀疏点云数据的处理中具有优势。

---

**3DPointCapsNet**

* **输入**：点云数据，每个点的特征（如位置、颜色等）。
* **输出**：全局潜在表示，经过多次最大池化层提取后的点云特征。
* **作用**：3DPointCapsNet 通过逐点的MLP和卷积层学习点独立的特征，并使用多个最大池化层提取全局潜在表示。该网络基于自监督的动态路由机制，学习强大的代表性潜在胶囊特征，有助于提取3D点云中的语义信息。

---

 **PointDAN (Point Domain Adaptation Network)**

* **输入**：点云数据，点云的不同领域之间的转换信息。
* **输出**：3D点云的表示，适用于领域适应的任务。
* **作用**：PointDAN 是一个端到端的无监督领域适应网络，用于3D点云表示。通过自监督方法，PointDAN可以在源领域和目标领域之间进行适应，并学习到从一个领域到另一个领域的特征转换，增强点云在不同领域下的表现力。

---

**自监督点云重建方法**

* **输入**：点云数据，部分点云被随机重排。
* **输出**：重建后的点云，恢复原始点云结构。
* **作用**：该方法通过自监督学习来捕捉点云的语义特性，特别是对被随机重排的点云部分进行重建。通过这种方式，网络能够学习点云中各部分之间的内在关系，并用于点云的重建任务。

---

**PointAugment**

* **输入**：点云数据，点云样本。
* **输出**：增强后的点云样本，用于网络训练。
* **作用**：PointAugment 是一种自动增强框架，旨在优化和增强点云样本以进行网络训练。该方法通过自动学习每个输入样本的形状变换和逐点位移，交替优化和更新增强器和分类器的可学习参数，提升点云数据增强的效率和准确性。

---

 **ShapeContextNet**

* **输入**：点云数据，点云的亲和点选择信息。
* **输出**：通过软对齐操作得到的点云特征。
* **作用**：ShapeContextNet 结合亲和点选择和紧凑特征聚合，利用点积自注意力机制进行软对齐操作。该方法受形状上下文启发，能够有效地处理点云中的空间分布和几何特征，并优化点云的对齐和匹配。

---

**4D旋转不变描述符**

* **输入**：点云数据，通过手工设计的点对函数生成的4D旋转不变描述符。
* **输出**：通过4D卷积神经网络处理后的点云特征。
* **作用**：Bobkov等人提出的该方法，利用基于手工特征点对函数的4D旋转不变描述符处理点云。通过4D卷积神经网络，能够有效处理3D点云中的噪声和遮挡，增强点云的旋转不变特性。

---

**Prokudin的点云编码方法**

* **输入**：点云数据，随机从单位球中均匀分布采样的基点集。
* **输出**：点云的编码表示，转化为固定长度的向量。
* **作用**：Prokudin等人提出的方法通过从单位球中均匀分布地随机采样基点集，并计算点云与基点集的最小距离，将点云编码为一个固定长度的向量。这种编码方式能够将点云转换为一个较小的表示，便于后续的机器学习方法处理。

---

**RCNet**

* **输入**：点云数据，点云分割后的平行束数据。
* **输出**：通过RNN和2D CNN进一步处理得到的点云特征。
* **作用**：RCNet 使用标准RNN和2D CNN构建了一个排列不变的网络来处理3D点云数据。首先，将点云分割为平行束并沿特定维度排序，然后通过共享的RNN进一步学习特征，再通过2D CNN进行特征聚合。RCNet-E则通过不同的分割和排序方向集成多个RCNet，以增强描述能力。

---

 **Point2Sequences**

* **输入**：点云数据，局部区域特征。
* **输出**：通过RNN编码器-解码器结构学习得到的点云特征。
* **作用**：Point2Sequences 是基于RNN的模型，捕捉点云局部区域之间的相关性。它将不同尺度的局部区域特征视为序列，并通过RNN编码器-解码器结构聚合这些局部区域特征，适用于捕捉点云中不同区域之间的关系。

---

**PVNet**

* **输入**：多视角图像，点云数据。
* **输出**：通过多视角图像与点云特征融合后的点云特征。
* **作用**：PVNet 通过从多视角图像中提取的高级全局特征，使用嵌入网络将图像特征投影到点云的子空间，并通过软注意力掩码与点云特征融合。最后，通过残差连接将融合特征与多视角特征结合，以进行形状识别。

---

**PVRNet**

* **输入**：3D点云数据及其多视图。
* **输出**：增强后的点云特征，融合单视图和多视图特征。
* **作用**：PVRNet 通过关系评分模块探索3D点云与其多视角之间的关系。基于关系评分，增强原始2D全局视图特征，并进行点单视图融合和点多视图融合，从而提高点云的表示能力。

‍

### <span data-type="text" style="background-color: var(--b3-card-error-background); color: var(--b3-card-error-color);">3D物体检测与跟踪：</span>

KITTI是最具影响力的数据集之一

#### （1）3D物体检测

一个典型的3D物体检测器以场景的点云为输入，并在每个检测到的物体周围生成一个定向的3D边界框，3D物体检测方法可以分为两类：基于区域提议的方法和单次检测方法。

##### ①基于区域提议的方法

这些方法<u>首先提出几个可能包含物体的区域（也称为提议）</u>，然后提取<u>每个区域的特征</u>，以确定每个提议的<u>类别标签</u>。根据物体提议生成的方法，这些方法可以进一步分为三类：基于多视图的、基于分割的和基于锥体的。

###### a）基于多视图的方法：

 **Chen等人的多视图3D物体检测方法**

* **输入**：BEV图，激光雷达前视图图像，RGB图像。
* **输出**：定向的3D边界框。
* **作用**：Chen等人从BEV图生成高精度的3D候选框，并将其投影到多个视角的特征图（如激光雷达前视图图像和RGB图像）上，结合这些来自不同视角的区域特征，预测定向的3D边界框。尽管该方法在召回率上达到了99.1%，但速度较慢，不适用于实际应用。

**Ku等人的多模态融合方法**

* **输入**：BEV视图和图像视图的特征。
* **输出**：融合后的3D物体提议。
* **作用**：Ku等人提出了一种基于多模态融合的区域提议网络，通过裁剪和调整大小操作从BEV和图像视图中提取相同大小的特征，并使用逐元素均值池化将这些特征融合，从而生成具有高召回率的小物体的3D提议。

**Liang等人利用连续卷积的方法**

* **输入**：图像和激光雷达特征图。
* **输出**：密集的BEV特征图。
* **作用**：Liang等人利用连续卷积在不同分辨率下有效融合图像和3D激光雷达特征图。他们提取BEV空间中每个点最接近的图像特征，并通过双线性插值将图像特征投影到BEV平面，最终获得密集的BEV特征图。实验结果表明，密集的BEV特征图比离散的图像特征图和稀疏的激光雷达特征图更适合3D物体检测。

**Liang等人的多任务多传感器网络**

* **输入**：点云数据，来自不同传感器（例如激光雷达和摄像头）的数据。
* **输出**：高精度的物体检测结果。
* **作用**：Liang等人提出的多任务多传感器3D物体检测网络，通过端到端训练，利用多个任务（如2D物体检测、地面估计和深度补全）帮助网络学习更好的特征表示。学习到的跨模态表示进一步用于生成高精度的物体检测结果。

**Zeng等人的预RoI池化卷积方法**

* **输入**：点云数据和物体提议。
* **输出**：高效的物体检测提议。
* **作用**：Zeng等人通过预RoI池化卷积提高了多视图方法的效率。将大部分卷积操作提前到RoI池化模块之前，从而对所有物体提议只进行一次RoI卷积，提高了计算效率，实验结果表明速度比MV3D快了5倍。

###### b）基于分割的方法：

**Yang等人利用2D分割网络的方法**

* **输入**：点云数据和2D分割结果。
* **输出**：高质量的3D物体提议。
* **作用**：Yang等人使用2D分割网络预测前景像素，并将其投影到点云中以去除大部分背景点。然后，他们在预测的前景点上生成物体提议，并设计了一个新的标准——PointsIoU，用于减少提议的冗余性和模糊性。

 **Shi等人的PointRCNN框架**

* **输入**：3D点云数据。
* **输出**：高质量的3D边界框。
* **作用**：Shi等人提出的PointRCNN框架直接对3D点云进行分割，以获得前景点，然后融合语义特征和局部空间特征，生成高质量的3D边界框

**Graph Convolution Network (GCN) 提议优化**

* **输入**：物体提议，图卷积网络。
* **输出**：优化后的物体提议。
* **作用**：Jesus等人提出了利用图卷积网络（GCN）优化3D物体提议的方法，通过R-GCN和C-GCN两个模块，分别对每个提议的特征进行聚合和上下文回归，从而提高物体检测的准确性。

---

###### c）基于锥体的方法：

 **F-PointNets**

* **输入**：2D图像区域，点云数据。
* **输出**：3D物体边界框。
* **作用**：F-PointNets为每个2D区域生成一个锥体提议，并应用PointNet（或PointNet++）来学习每个3D锥体的点云特征，进行3D框估计。

**Zhao等人的Point-SENet模块**

* **输入**：3D点云数据。
* **输出**：经过缩放调整的有用特征。
* **作用**：Zhao等人提出的Point-SENet模块用于预测缩放因子，以自适应地突出有用特征并抑制无信息特征，增强3D物体检测性能。

**Shin等人的3D候选框提取方法**

* **输入**：2D图像，3D物体姿态数据。
* **输出**：多个几何上可行的3D候选框。
* **作用**：Shin等人首先从2D图像中估计物体的2D边界框和3D姿态，然后提取多个几何上可行的物体候选框，进一步回归到准确的3D边界框。

**Wang等人的锥体序列生成方法**

* **输入**：2D图像区域，点云数据。
* **输出**：3D物体边界框。
* **作用**：Wang等人为每个2D区域生成一系列锥体，通过PointNet提取每个锥体的特征，再进行3D框估计。

###### d）其他方法

**Zhou等人的3D旋转IoU方法**

* **输入**：旋转3D边界框。
* **输出**：提升性能的3D物体检测结果。
* **作用**：Zhou等人将3D旋转边界框的IoU整合到多个最先进的检测器中，实现了性能的一致提升。

**Shi等人的PointVoxel-RCNN**

* **输入**：点云数据。
* **输出**：高质量的3D物体提议。
* **作用**：通过体素化点云数据，并结合3D稀疏卷积网络和PointNet基于集合抽象模块学习点云特征，进一步生成高质量的3D物体提议。

**Qi等人的VoteNet**

* **输出**：高质量的3D物体提议。

* **输入**：点云数据。
* **作用**：VoteNet 通过投票方法生成虚拟中心点的物体提议，进一步增强3D物体检测性能。

**VoteNet**

* **输入**：3D点云数据。
* **输出**：通过聚合投票特征生成的一组高质量3D物体提议。
* **作用**：VoteNet 受霍夫投票法启发，直接对物体的虚拟中心点进行投票，并通过聚合投票特征生成3D物体提议。该方法只使用几何信息，显著超越了先前的方法，并在ScanNet和SUN RGB-D这两个大型室内基准上取得了最先进的性能。然而，该方法对于部分遮挡的物体，虚拟中心点的预测存在不稳定的问题。

**Feng等人提出的改进方法**

* **输入**：3D点云数据，虚拟中心点的预测。
* **输出**：改进后的虚拟中心点和3D候选框的预测结果。
* **作用**：Feng等人增加了一个方向向量的辅助分支，用以提高虚拟中心点和3D候选框的预测精度。此外，他们还建立了物体-物体关系图，以强调有用特征，从而提高物体检测的准确性。

 **ImVoteNet**

* **输入**：3D点云数据，2D物体检测的提示信息（例如几何特征和语义/纹理特征）。
* **输出**：通过融合2D物体检测提示生成的高质量3D物体提议。
* **作用**：Qi等人提出的ImVoteNet通过将2D物体检测提示（如几何特征和语义特征）融合到3D投票管道中，从而生成3D物体提议，进一步提升物体检测的性能。

**Part-A2 Net**

* **输入**：3D点云数据。
* **输出**：3D物体边界框，内部分部件位置的预测。
* **作用**：Shi等人提出的Part-A2 Net 由部件感知阶段和部件聚合阶段组成。部件感知阶段应用类似UNet的网络，结合稀疏卷积和稀疏反卷积，学习逐点特征，用于预测和生成物体内部部件的位置。部件聚合阶段采用RoI感知池化，将预测的部件位置聚合，并进行框精细化，最终输出物体的3D边界框。该方法适用于从3D物体中提取精细的部件信息，并提高检测的准确性。

##### ②单次检测方法

这些方法通过一个<u>单阶段网络直接预测物体的类别概率</u>，并回归<u>3D边界框</u>。它们不需要生成区域提议和后处理，因此可以实现较高的运行速度。根据输入数据的类型，单次检测方法可以分为三类：基于BEV的方法、基于离散化的方法和基于点的方法。

###### a）基于BEV的方法

**Yang等人的BEV方法**

* **输入**：激光雷达点云数据（BEV表示）。
* **输出**：3D物体的边界框和朝向角度。
* **作用**：Yang等人将场景的点云离散化为等间距的网格，并通过全卷积网络（FCN）估计物体位置和朝向角度。该方法速度较快，但对不同密度点云的泛化能力较弱。为此，提出了结合HD地图进行地图感知的方法，大大提高了鲁棒性。

**Beltran等人的归一化地图方法**

* **输入**：激光雷达点云数据，HD地图。
* **输出**：增强泛化能力的BEV表示。
* **作用**：Beltran等人提出的归一化地图方法通过考虑不同激光雷达传感器之间的差异，使用2D网格编码每个单元格最大点数，显著提升了基于BEV的检测器的泛化能力，尤其适用于多种传感器环境下。

###### b）基于离散化的方法

**Li等人基于FCN的离散化方法**

* **输入**：3D点云数据。
* **输出**：3D物体的边界框和置信度。
* **作用**：Li等人提出了将点云离散化为2D点图并使用FCN预测3D物体边界框和置信度。该方法通过将3D点云转换为2D图像形式，简化了计算过程，适用于较快的3D物体检测。

**Engelcke等人的投票方案**

* **输入**：稀疏体素（3D体素化的点云）。
* **输出**：通过投票生成的3D物体检测结果。
* **作用**：Engelcke等人提出的以特征为中心的投票方案，通过为每个非空体素生成投票并累积投票结果，解决了体素稀疏性的问题，从而提高了检测性能和计算效率。

**VoxelNet**

* **输入**：体素化的点云数据。
* **输出**：3D物体检测结果。
* **作用**：Zhou等人提出的VoxelNet将点云分割成等间距的体素并编码特征，连接区域提议网络（RPN）生成检测结果。该方法性能优越，但因体素稀疏性和3D卷积的计算复杂性，运行速度较慢。

**Sindagi等人的多模态融合方法**

* **输入**：图像和点云数据。
* **输出**：融合后的准确3D边界框。
* **作用**：Sindagi等人通过将非空体素投影到图像上，并通过预训练网络提取图像特征，结合点云体素特征，生成准确的3D框。该方法有效地利用了多模态信息，减少了假阳性和假阴性。

**PointPillars**

* **输入**：点云数据（Pillars形式的点云特征）。
* **输出**：3D物体边界框。
* **作用**：Lang等人提出的PointPillars方法利用PointNet学习垂直列（Pillars）中组织的点云特征，并将特征编码为伪图像，然后应用2D物体检测管道预测3D边界框。PointPillars在速度和精度上表现出色，适用于实际应用。

###### c）基于点的方法

**3DSSD**

* **输入**：原始3D点云数据。
* **输出**：3D物体边界框。
* **作用**：3DSSD通过融合距离-FPS（D-FPS）和特征-FPS（F-FPS）采样策略，去除了时间消耗较大的特征传播层和精细化模块，使用无锚点的回归头结合3D中心度标签来预测3D物体框。实验结果显示，3DSSD在速度和精度上优于PointRCNN【133】。

###### d）其他方法

**LaserNet**

* **输入**：点云的密集范围视图（RV表示）。
* **输出**：3D物体边界框。
* **作用**：Meyer等人提出的LaserNet方法通过为每个点预测边界框的概率分布，并结合这些分布生成最终的3D物体框。该方法使用快速均值偏移算法减少预测噪声，在0到50米范围内实现了最先进的性能，且运行时间显著低于其他方法。

**LaserNet的RGB图像扩展**

* **输入**：LiDAR点云，RGB图像。
* **输出**：融合后的准确3D边界框。
* **作用**：Meyer等人扩展了LaserNet，利用RGB图像的纹理信息与LiDAR点云融合，从而提高了50到70米范围内物体检测的性能，并在3D语义分割任务中显著提升了效率。

**基于热点的检测方法**

* **输入**：体素化的点云数据。
* **输出**：3D物体边界框。
* **作用**：Chen等人提出的基于热点的无锚点检测器方法通过体素化点云并输入骨干网络，生成3D特征图，进而分类热点并预测3D边界框。该方法在稀疏点云中表现强大且与其他方法具有可比性。

**Point-GNN**

* **输入**：激光雷达点云数据。
* **输出**：物体类别和3D边界框。
* **作用**：Shi等人提出的Point-GNN通过将点云编码为邻近图并输入到图神经网络中，预测物体的类别和边界框。这种方法能够有效处理点云数据中的复杂关系，提升了物体检测的性能。

**VoteNet**

* **输入**：3D点云数据。
* **输出**：通过投票生成的高质量3D物体提议。
* **作用**：VoteNet 受霍夫投票法启发，直接从点云中对物体的虚拟中心点进行投票，并通过聚合投票特征生成3D物体提议。该方法通过使用几何信息显著超越了先前的方法，尤其在ScanNet和SUN RGB-D数据集上取得了最先进的性能。它的优势在于使用简单的几何特征，但对于部分遮挡的物体，虚拟中心点的预测不稳定。

**Feng等人提出的改进方法**

* **输入**：3D点云数据，虚拟中心点的预测。
* **输出**：改进后的虚拟中心点和3D候选框。
* **作用**：Feng等人增加了方向向量辅助分支，提升了虚拟中心点和3D候选框的预测精度，并建立了物体-物体关系图来提高检测精度。

**ImVoteNet**

* **输入**：3D点云数据，2D物体检测的提示（如几何特征和语义/纹理特征）。
* **输出**：通过融合2D物体检测提示生成的3D物体提议。
* **作用**：Qi等人提出的ImVoteNet通过将2D物体检测提示（如几何和语义特征）融合到3D投票管道中，从而生成3D物体提议。该方法有效提升了物体检测的性能，尤其在多模态融合方面表现优异。

**Part-A2 Net**

* **输入**：3D点云数据。
* **输出**：物体的3D边界框，内部部件位置的预测。
* **作用**：Shi等人提出的Part-A2 Net，包含部件感知和部件聚合两个阶段。通过UNet-like网络学习点云的逐点特征，预测物体内部部件的位置，并进行框精细化。该方法有效提升了检测精度，尤其是在复杂物体的细节检测上。

#### （2）3D物体跟踪

给定物体在第一帧的位置，物体追踪的任务是估计其在后续帧中的状态，由于3D物体追踪可以利用点云中的丰富几何信息，因此预计它能够克服基于图像的追踪方法面临的几个缺点，包括遮挡、光照变化和尺度变化。

**3D Siamese Network with Shape Completion Regularization (Giancola et al. 2019)**

* **中文名**：带形状完成正则化的3D Siamese网络
* **作用**：该方法基于Siamese网络架构，结合形状完成正则化，提出了一种新的3D物体追踪方式。它利用卡尔曼滤波器生成候选框，并通过形状正则化来编码模型和候选框。通过余弦相似度，在下一帧中寻找目标物体的位置。
* **输入**：第一帧中的物体位置（通过卡尔曼滤波器生成候选框）和形状正则化的模型。
* **输出**：下一帧中物体的位置。
* **优势**：该方法显著优于大多数2D物体追踪方法，能够克服遮挡、光照变化和尺度变化的挑战。

**2D Siamese Network for Coarse Candidate Generation and 3D Siamese Network for Refinement (Zarzar et al. 2020)**

* **中文名**：基于2D Siamese网络和3D Siamese网络的物体追踪方法
* **作用**：为了高效搜索目标物体，Zarzar等人利用2D Siamese网络在BEV（鸟瞰图）表示上生成大量粗略的物体候选框，然后利用3D Siamese网络中的余弦相似度来精细化这些候选框。
* **输入**：来自2D Siamese网络生成的粗略物体候选框（BEV表示）。
* **输出**：精细化的物体候选框（3D位置）。
* **优势**：该方法在精度上比Giancola等人的方法提高了18%，在成功率上提高了12%。

**3D Object Detection and Tracking Architecture for Semantic Point Clouds (Simon et al. 2020)**

* **中文名**：语义点云的3D物体检测与追踪架构
* **作用**：该方法提出了一种针对语义点云的3D物体检测与追踪架构。首先通过融合2D视觉语义信息生成体素化的语义点云，然后利用时间信息提高多目标追踪的准确性和鲁棒性。
* **输入**：2D视觉语义信息和点云数据。
* **输出**：体素化的语义点云，并利用时间信息优化多目标追踪。
* **优势**：引入了简化的评估指标（尺度-旋转-平移分数SRFs），加速了训练和推理，且Complexer-YOLO实现了实时追踪性能。

**Point-to-Box (P2B) Network (Qi et al. 2020)**

* **中文名**：Point-to-Box（P2B）网络
* **作用**：Qi等人提出了Point-to-Box (P2B) 网络，用于3D物体追踪。该方法通过将模板和搜索区域输入到骨干网络中，获得它们的种子。然后，搜索区域的种子通过目标特定特征增强，再通过霍夫投票回归潜在的目标中心。
* **输入**：模板和搜索区域。
* **输出**：目标中心的位置，3D物体框的回归结果。
* **优势**：实验结果显示，P2B在40帧每秒的速度下，比Giancola等人提出的方法（3D Siamese Network）提高了超过10%的性能，具有显著的速度和精度优势。

#### （3）3D场景流估计

给定两个点云 X 和 Y，3D场景流 D 描述了每个点 xi 在 X 中的移动到其对应位置 x’i 在 Y 中的过程，即 x’i \= xi + di。

**FlowNet3D (Liu et al., 2019)**

* **作用**：FlowNet3D 是一种直接从一对连续点云中学习场景流的方法。它通过流嵌入层同时学习点级特征和运动特征，以估计3D场景流。
* **输入**：一对连续的点云数据（X 和 Y），每个点云包含一组3D坐标。
* **输出**：每个点的运动向量（场景流），即从点云 X 到 Y 的位移（di）。
* **问题与改进**：

  * **问题1**：一些预测的运动向量在方向上与地面真值有显著差异。
  * **问题2**：在动态场景中应用困难，尤其是对于由可变形物体主导的场景。
  * **改进**：Wang等人引入了余弦距离损失（最小化预测值与真值之间的角度）和点到平面距离损失（提高刚性和动态场景中的准确性）。这两个损失项提升了FlowNet3D的准确性，从57.85%提高到63.43%，并加速了训练过程。

**Hierarchical Permutohedral Lattice FlowNet (HPLFlowNet) (Gu et al., 2020)**

* **作用**：HPLFlowNet旨在直接从大规模点云中估计场景流。通过多个双边卷积层，该方法恢复了点云的结构信息，同时减少了计算成本。
* **输入**：大规模的点云数据。
* **输出**：每个点的场景流（位移）。
* **优势**：该方法通过双边卷积层高效地恢复结构信息，适用于大规模点云。

**PointRNN, PointGRU, PointLSTM (Fan and Yang, 2020)**

* **作用**：这些网络（PointRNN, PointGRU, PointLSTM）通过序列到序列的模型来处理连续的点云数据，用于追踪动态点。它们能够捕捉时空信息，并建模动态点云。
* **输入**：连续的点云序列，包含时间序列信息和每一帧的3D坐标。
* **输出**：每个点在序列中的移动轨迹，动态点云的估计结果。
* **优势**：这些方法有效地捕捉了时空信息，能够跟踪和预测动态点云的变化。

**MeteorNet (Liu et al., 2020)**

* **作用**：MeteorNet直接从动态点云中学习表示，并通过时空邻近点的信息聚合来估计场景流。它引入了直接分组和链式流分组方法来确定时间邻居，从而改进了场景流估计。
* **输入**：动态点云数据。
* **输出**：每个点的运动（场景流），即点云中各点的位移。
* **优势**：该方法通过有效地聚合时空邻近点的信息，改进了动态点云的场景流估计，但受限于数据集的规模。

**Self-Supervised Scene Flow Estimation (Mittal et al., 2020)**

* **作用**：Mittal等人提出了自监督损失，用于在大规模无标签数据集上训练场景流估计网络。他们的方法包括前向和反向预测，通过计算反向方向的场景流并使用循环一致性损失来确保估计的正确性。
* **输入**：未经标注的点云数据集，以及通过预测转换的点云。
* **输出**：每个点的运动向量（场景流）。
* **优势**：该方法通过自监督学习有效估计场景流，无需人工标注的场景流数据，且超越了基于监督学习方法的最先进性能。

  * **自监督方法**：利用反向预测并计算循环一致性损失，以避免真值和预测之间的差异。

‍

‍

‍

### <span data-type="text" style="background-color: var(--b3-card-error-background); color: var(--b3-card-error-color);">3D点云分割</span>

根据分割的粒度，3D点云分割方法可以分为三类：语义分割（场景级别）、实例分割（物体级别）和部件分割（部件级别）。

#### （1）语义分割（场景级别）

给定一个点云，语义分割的目标是<u>根据点的语义含义将其划分为若干子集</u>。类似于3D形状分类的分类方法，语义分割有四种<u>范式</u>：基于投影的、基于离散化的、基于点的和混合方法。

无论是投影方法还是离散化方法，第一步都是<u>将点云转换为中间的规则表示</u>，然后，<u>中间的分割结果被投影回原始点云</u>。相比之下，基于点的方法直接处理不规则的点云。

‍

**基于投影的方法**和**基于离散化的方法**都能借鉴成熟的2D图像网络架构。然而：

* 基于投影方法的主要局限在于<u>3D到2D投影导致的信息丢失</u>
* 基于离散化方法的主要瓶颈是<u>分辨率提升</u>带来的<u>立方级计算和内存开销</u>  
  对此，**基于索引结构的稀疏卷积**可能是可行解决方案，值得深入探索。

**基于点的方法**是目前研究最广泛的方向，但点表示天然缺乏显式邻域信息，现有方法多依赖昂贵的邻域搜索机制（如KNN或球形查询），这本质上限制了效率。

‍

##### ①基于投影的方法

###### a）多视图表示

**多流FCN（Fully Convolutional Network）**

* **功能**: 多流FCN 是一种全卷积网络（FCN），用于在2D合成图像上进行像素级预测。在多视图分割中，多个虚拟摄像头的视角将3D点云投影到2D图像上，网络在这些图像上进行处理并预测每个像素的得分。
* **输入**: 从多个虚拟摄像头视角获得的2D投影图像（这些图像可能是RGB图像或深度图像）。
* **输出**: 每个像素的得分，之后通过不同视角的得分融合来生成最终的语义标签。每个点的最终语义标签指示该点属于哪个类别（例如道路、建筑物等）。

**2D分割网络（2D Segmentation Networks）**

* **功能**: 使用2D分割网络对从多个视角生成的RGB和深度快照进行像素级标注。这些网络通常采用传统的CNN架构或其他卷积神经网络来执行语义分割任务。
* **输入**: 从多个摄像机位置生成的RGB和深度图像快照。
* **输出**: 每个像素的类别标签，表示图像中每个点的语义类别。经过进一步的融合，这些标签转移到3D点云中。

**残差修正（Residual Correction）**

* **功能**: 残差修正用于对来自不同视角的图像的分割结果进行精细化融合。它通过修正不同视角图像之间的差异来提高分割精度，特别是处理投影和融合过程中出现的误差。
* **输入**: 从不同视角（RGB和深度图像）获得的分割得分。
* **输出**: 更精确的语义分割结果，通常用于消除残差和修正不一致的分割输出。

**切线卷积（Tangent Convolutions）**

* **功能**: 切线卷积是一种处理3D点云的卷积方法，通过将点云的局部几何结构投影到虚拟切线平面上进行卷积操作。它旨在更好地处理密集点云，能够有效提取局部的几何信息并进行语义分割。
* **输入**: 3D点云数据及其局部几何结构信息。
* **输出**: 每个点的语义标签，指示该点的类别。该方法适用于大规模点云，并且可以在处理数百万点的点云时保持良好的性能。

###### b）球形表示

**SqueezeNet**

* **功能**: SqueezeNet 是一种轻量级的神经网络架构，专为减少模型大小而设计，同时保持较高的性能。它被用于3D点云分割任务中，通常与其他模型（如条件随机场CRF）结合，以提高分割精度。
* **输入**: 转换为球形表示的3D点云数据。
* **输出**: 3D点云中每个点的语义标签，用于指示该点的类别。

**条件随机场（Conditional Random Field, CRF）**

* **功能**: CRF 被用于进一步优化语义分割的结果，特别是在像素级别的分割中。它考虑到点云中不同区域之间的空间关系，以提高分割的准确性。
* **输入**: 来自SqueezeNet等网络的初步分割结果。
* **输出**: 优化后的分割结果，减少误差并提高精度。

**RangeNet++**

* **功能**: RangeNet++ 是一种实时语义分割方法，专门用于激光雷达点云。它通过将2D范围图像的语义标签转移到3D点云上，并使用GPU加速的KNN后处理步骤来减少离散化误差和模糊推断输出。
* **输入**: 激光雷达点云数据以及对应的2D范围图像。
* **输出**: 每个点的语义标签，提供精确的点云分割结果。

##### ②基于离散化的方法

这些方法通常将点云转换为<u>稠密/稀疏的离散表示</u>。

###### a）稠密离散化表示

**3D卷积神经网络（CNN）**

* **功能**: 用于处理体素化的点云数据，通过卷积操作对3D点云进行分割和特征提取。
* **输入**: 体素化后的点云（即3D网格），每个体素内包含点云数据。
* **输出**: 每个体素的语义标签。

**SEGCloud**

* **功能**: 实现精细且全局一致的语义分割，通过三线性插值和全连接条件随机场（FC-CRF）来优化每个点的标签。
* **输入**: 由3D-FCNN生成的粗体素预测，点云数据。
* **输出**: 每个点的精确语义标签。

**变分自编码器（VAE）**

* **功能**: 编码每个体素内的点云分布，生成紧凑的潜在空间表示，用于特征学习。
* **输入**: 每个体素内的点云分布（通过径向基函数（RBF）表示）。
* **输出**: 紧凑的潜在空间表示，用于后续的特征学习。

**Fully-Convolutional Point Network (FCPN)**

* **功能**: 使用3D卷积和加权平均池化从点云中抽象不同级别的几何关系，处理大规模点云数据。
* **输入**: 点云数据。
* **输出**: 提取的特征，带有长距离依赖的语义标签。

**ScanComplete**

* **功能**: 实现3D扫描补全和每体素的语义标注，利用完全卷积神经网络的可扩展性。
* **输入**: 点云数据，体素化的点云。
* **输出**: 补全的3D扫描，带有每体素的语义标签。

###### b）稀疏离散化表示

**子流形稀疏卷积网络（Submanifold Sparse Convolutional Networks）**

* **功能**: 限制卷积的输出只与占用体素相关，减少内存和计算成本，处理稀疏点云数据。
* **输入**: 稀疏点云数据，体素表示。
* **输出**: 稀疏卷积后的特征，进一步用于点云分割。

**MinkowskiNet**

* **功能**: 处理3D视频感知，通过4D时空卷积神经网络来处理高维和空间稀疏数据，并强制一致性。
* **输入**: 时空点云数据。
* **输出**: 处理后的特征，保持一致性，适用于时空分析。

**SPLATNet (Sparse Lattice Networks)**

* **功能**: 将原始点云插值到Permutohedral稀疏晶格中，在占用部分应用双边卷积层（BCLs）进行卷积操作，再将结果回投影到原始点云。
* **输入**: 原始点云数据。
* **输出**: 占用部分卷积后的特征，回投影到点云中，最终的语义标签。

**LatticeNet**

* **功能**: 处理大规模点云数据，提供一个高效的数据依赖插值模块（DeformsSlice）来将晶格特征回投影到点云中。
* **输入**: 稀疏晶格和点云数据。
* **输出**: 处理后的点云特征，并回投影到点云中，优化后的特征和标签。

##### ③混合方法

**Joint 3D-Multi-View Network**（Dai & Nießner, [190]）

* **功能**：结合RGB特征与几何特征，实现多模态特征融合。
* **输入**：

  * 3D扫描数据（点云/网格）
  * 多视角RGB图像
* **输出**：融合后的3D几何与2D纹理特征
* **核心组件**：

  * **3D CNN流**：处理3D几何数据
  * **2D流（多个）** ：提取多视角图像特征
  * **可微分反投影层**：将2D特征映射回3D空间并融合

**Unified Point-Based Framework**（Chiang et al., [200]）

* **功能**：直接从点云学习2D纹理、3D结构及全局上下文特征。
* **输入**：稀疏采样的点云数据
* **输出**：包含局部几何和全局语义的点云特征
* **核心特点**：

  * 无需体素化（voxelization）
  * 直接处理原始点云

**Multi-view PointNet (MVPNet)** （Jaritz et al., [191]）

* **功能**：聚合2D多视角图像的外观特征与3D点云的空间几何特征。
* **输入**：

  * 多视角RGB图像
  * 规范化的3D点云
* **输出**：融合后的3D语义点云特征
* **核心机制**：

  * 2D CNN提取多视角图像特征
  * 点云空间的特征对齐与融合

##### ④基于点的方法

基于PointNet，近期涌现了一系列点云网络方法，总体上可分为以下四类：逐点MLP方法、点卷积方法、基于RNN的方法和基于图的方法。

###### a）逐点MLP方法

**PointNet** 

* **功能**：处理无序点云数据，学习逐点特征和全局特征
* **输入**：原始点云（N×3坐标，可选附加特征如颜色）
* **输出**：逐点特征 + 全局特征向量
* **核心操作**：共享MLP + 对称池化（如max pooling）

**PointNet++**  

* **功能**：分层学习局部几何结构
* **输入**：原始点云
* **输出**：多尺度局部特征
* **关键模块**：

  * **多尺度分组（MSG）** ：不同半径邻域查询
  * **多分辨率分组（MRG）** ：组合不同层级特征

**PointSIFT  / 点云方向感知模块**

* **功能**：方向感知的局部特征提取
* **输入**：点云局部区域
* **输出**：方向编码特征
* **核心操作**：三阶段有序卷积（八方向）

**PointWeb / PointWeb with Adaptive Feature Adjustment / 带自适应特征调整的点网状网络**

* **功能**：局部点间关系建模
* **输入**：点云局部区域
* **输出**：自适应调整后的特征
* **关键模块**：局部全连接网络 + 特征调整模块

 **ShellConv / Permutation-Invariant Shell Convolution / 置换不变的球壳卷积**

* **功能**：置换不变卷积
* **输入**：点云局部区域
* **输出**：球壳统计特征
* **核心操作**：同心球壳max pooling + 1D卷积

**RandLA-Net / Random Sampling and Local Feature Aggregation Network / 随机采样与局部特征聚合网络**

* **功能**：大规模点云高效分割
* **输入**：大规模点云
* **输出**：逐点语义标签
* **关键模块**：

  * 随机点采样
  * 局部特征聚合模块

###### b）点卷积方法

**PCCN / Parametric Continuous Convolution Network / 参数化连续卷积网络**

* **功能**：连续空间卷积
* **输入**：点云
* **输出**：卷积后特征
* **核心操作**：MLP参数化的连续卷积核

**KP-FCNN / Kernel Point Fully Convolutional Network / 核点全卷积网络**

* **功能**：动态卷积
* **输入**：点云
* **输出**：卷积后特征
* **关键模块**：

  * **KPConv**：基于核点距离的权重分配
  * 半径邻域查询

**DPC / Dilated Point Convolution / 膨胀点卷积**

* **功能**：膨胀邻域特征聚合
* **输入**：点云特征
* **输出**：大感受野特征
* **核心操作**：膨胀邻域替换KNN

###### c）基于RNN的方法

**RCU / Recurrent Consolidation Unit / 循环巩固单元**

* **功能**：序列化上下文建模
* **输入**：PointNet提取的块特征
* **输出**：时序增强特征
* **核心操作**：循环巩固单元（LSTM/GRU）

**3P-HRNN / Pointwise Pyramid Pooling with Hierarchical RNNs / 点向金字塔池化+分层循环神经网络**

* **功能**：长程依赖捕获
* **输入**：点云
* **输出**：层次化特征
* **关键模块**：

  * 点向金字塔池化（3P）
  * 双向分层RNN

**DAR-Net / Dynamic Aggregation Network / 动态聚合网络**

* **功能**：动态感受野调整
* **输入**：点云
* **输出**：自适应聚合特征
* **核心操作**：自适应的节点权重

###### d）基于图的方法

**Superpoint Graph / Superpoint Graph with Attributed Directed Edges / 带属性有向边的超点图**

* **功能**：结构上下文建模
* **输入**：点云
* **输出**：有向图表示
* **关键操作**：

  * 几何超点分割
  * 图卷积

**GAC / Graph Attention Convolution / 图注意力卷积**

* **功能**：注意力图卷积
* **输入**：点云局部图
* **输出**：注意力加权特征
* **核心操作**：空间/特征双注意力

**PointGCR / Point Global Context Reasoning Module / 点云全局上下文推理模块**

* **功能**：全局上下文推理
* **输入**：点云特征
* **输出**：通道增强特征
* **关键模块**：无向图通道关系建模

###### e）其他方法（弱监督方法）

**Two-Stage Subcloud-Level Weakly Supervised Segmentation / 两阶段子云级弱监督分割方法**

* **功能**：子云级标签学习
* **输入**：弱标注点云
* **输出**：分割结果
* **核心策略**：两阶段训练

**Partially Labeled Point Cloud Segmentation with Inexact Supervision / 非精确监督的部分标注点云分割方法**

* **功能**：部分标注点学习
* **输入**：10%标注点云
* **输出**：完整分割结果
* **关键操作**：非精确监督损失

#### （2）实例分割（物体级别）

相较于语义分割，实例分割更具挑战性，因其需要对点云进行更精确、更细粒度的推理。该方法不仅需要区分不同语义类别的点，还需将同一语义类别的不同实例分离。现有方法可分为两类：<u>基于提案的方法</u>和<u>无提案的方法</u>。

**提案（Proposal）定义**：指候选检测框或分割区域，如GSPN输出的3D形状、3D-RPN生成的ROI等。

##### ①基于提案的方法

这类方法将实例分割任务拆解为两个子任务：<u>三维目标检测</u>与<u>实例掩码预测</u>。

**3D-SIS网络** 

* **英文全称**：3D Fully-Convolutional Semantic Instance Segmentation Network
* **功能**：RGB-D扫描数据的语义实例分割
* **输入**：RGB-D扫描数据（颜色+深度）
* **输出**：

  * 3D边界框位置
  * 物体类别标签
  * 实例掩码
* **核心模块**：

  * 3D-RPN（3D Region Proposal Network）
  * 3D-RoI（3D Region of Interest）层

---

**GSPN** 

* **英文全称**：Generative Shape Proposal Network
* **功能**：生成高质量3D提案
* **输入**：点云数据
* **输出**：高物体性（objectness）的3D形状提案
* **配套工具**：

  * **R-PointNet**：基于区域的PointNet，用于提案细化

---

**在线体素映射系统** 

* **功能**：联合实现大规模3D重建、语义标注和实例分割
* **输入**：2D图像序列
* **输出**：

  * 体素化3D地图
  * 语义标签
  * 实例分割结果
* **关键技术**：

  * 2D全景分割网络（未命名具体模型）
  * 全连接CRF（Conditional Random Field）

---

**3D-BoNet** 

* **英文全称**：3D Box Network
* **功能**：点云实例分割
* **输入**：原始点云
* **输出**：

  * 3D边界框（直接回归）
  * 实例标签（通过点级二分类器）
* **创新点**：

  * 将边界框生成建模为最优分配问题
  * 多标准损失函数

---

**LiDAR实例分割网络** 

* **功能**：大规模户外LiDAR点云实例分割
* **输入**：LiDAR点云（鸟瞰图表示）
* **输出**：实例标签
* **核心模块**：

  * 自注意力块（Self-Attention Blocks）
* **预测依据**：水平中心坐标 + 高度限制

---

**VDRAE** 

* **英文全称**：Variational Denoising Recursive AutoEncoder
* **功能**：室内3D空间布局预测与提案生成
* **输入**：室内点云
* **输出**：迭代优化的物体提案
* **技术特点**：

  * 递归上下文聚合与传播

##### ②无提案的方法

无提案方法不包含目标检测模块，而是将实例分割视为语义分割后的聚类步骤。这类方法基于核心假设：同一实例的点应具有高度相似的特征，因此主要关注<u>判别性特征学习</u>与<u>点聚类</u>。

相似性分组提案网络(SGPN) 

* **英文全称**: Similarity Group Proposal Network
* **核心功能**: 通过构建点对相似度矩阵实现实例分割
* **详细输入**:

  * 原始点云数据(N×3坐标，可包含RGB颜色/Normal等附加特征)
  * 点云场景的语义标签(训练时)
* **详细输出**:

  * 点级特征图(N×D维特征向量，D为特征维度)
  * 语义预测图(N×C矩阵，C为语义类别数)
  * N×N相似度矩阵(每个元素表示两点属于同一实例的概率)
* **关键技术组件**:

  * 双铰链损失函数:

    * 正样本对(同实例点对)相似度\>阈值T1
    * 负样本对(不同实例点对)相似度\<阈值T2
  * 非极大值抑制策略:

    * 基于相似度矩阵的贪心合并算法
    * 重叠率阈值控制合并过程
* **内存消耗**: 处理10000个点需约1.5GB显存(N×N矩阵)

子流形稀疏卷积 

* **英文全称**: Submanifold Sparse Convolution
* **核心功能**: 高效处理稀疏体素数据
* **详细输入**:

  * 体素化点云(通常0.05m体素分辨率)
  * 每个激活体素包含:

    * 3D坐标(x,y,z)
    * 特征向量(原始点特征均值)
* **详细输出**:

  * 体素级语义分数(每个体素的C类概率分布)
  * 邻域亲和力图(每个体素与6/26邻接体素的连接强度)
* **稀疏性处理**:

  * 仅计算非空体素的卷积
  * 使用哈希表存储激活体素
* **后续聚类**:

  * 基于亲和力的区域生长算法
  * 网格拓扑约束(防止跨表面连接)

PartNet分割检测网络 

* **主干架构**: PointNet++改进版
* **详细输入**:

  * 原始点云(2048个采样点)
  * 部件标注信息(训练时)
* **详细输出**:

  * 语义标签(每个点的部件类别)
  * 实例掩码(N×K矩阵，K为最大实例数)
* **关键改进**:

  * 多尺度特征融合(3级SA层)
  * 部件感知的采样策略
* **训练数据**:

  * PartNet数据集(57383个带部件标注的3D模型)

结构感知损失 

* **数学形式**:  
  L \= αL\_feat + βL\_geo

  * L\_feat: 特征相似性损失
  * L\_geo: 几何关系损失(距离/法向约束)
* **特征优化**:

  * 同实例点特征距离\<0.5
  * 不同实例点特征距离\>1.5
* **图CNN架构**:

  * 动态KNN构图(K\=20)
  * 多头注意力机制(4头)
  * 边特征更新公式:  
    e\_ij \= MLP([f\_i, f\_j, x\_i-x\_j])

ASIS联合分割模块 

* **特征融合方式**:

  * 语义→实例: 类别注意力门控
  * 实例→语义: 特征残差连接
* **训练参数**:

  * 初始学习率0.01
  * 动量0.9
  * 权重衰减0.0001
* **性能提升**:

  * ScanNet基准mAP提高2.3%

MTPNet多任务网络 

* **判别性损失**:  
  L\_dis \= max(0, m + D(f\_i,f\_j) - D(f\_i,f\_k))

  * m\=1.0为边界
  * f\_i,f\_j同实例
  * f\_i,f\_k不同实例
* **MV-CRF参数**:

  * 一元势能: 网络预测分数
  * 二元势能:  
    θ\_1 \= exp(-||f\_i-f\_j||\^2)  
    θ\_2 \= exp(-||x\_i-x\_j||\^2/σ\^2)
  * σ\=0.1为空间尺度参数

动态区域生长(DRG) 

* **补丁生成**:

  * 初始种子点选择: 曲率极值点
  * 生长准则:  
    |n\_i·n\_j|\>0.8  
    ||x\_i-x\_j||\<0.1m
* **K-means++改进**:

  * 初始中心间距约束
  * 自适应聚类数估计

混合2D-3D网络 

* **BEV特征提取**:

  * 分辨率0.1m/pixel
  * 特征通道数256
* **点云分支**:

  * 局部几何特征(FPN结构)
  * 关键点采样率5%

多任务学习框架 

* **方向预测**:

  * 单位向量场预测
  * 损失函数:  
    L\_dir \= 1 - cos(v\_pred, v\_gt)
* **特征维度**:

  * 实例嵌入: 32维
  * 方向向量: 3维

PointGroup 

* **双集聚类**:

  * 集合1: 原始坐标空间
  * 集合2: 偏移后坐标空间
  * 偏移量预测范围±0.5m
* **ScoreNet架构**:

  * 3层MLP
  * 输出[0,1]置信度分数
  * 训练正负样本比1:3

JSNet联合分割网络 

* **英文全称**: Joint Semantic and Instance Segmentation Network
* **核心架构**:

  * 双流特征提取器

    * 几何特征流：PointNet++变体
    * 语义特征流：稀疏3D CNN
* **详细输入**:

  * 点云坐标(x,y,z)
  * 可选RGB/强度特征
* **详细输出**:

  * 语义logits(N×C)
  * 实例嵌入(N×32)
* **创新点**:

  * 交叉流注意力模块
  * 动态特征门控机制
* **训练参数**:

  * 初始学习率0.005
  * batch size 16
  * 训练epoch 200

概率嵌入方法 

* **英文全称**: Probabilistic Embeddings for Point Cloud Segmentation
* **网络架构**:

  * 共享编码器：5层MLP(256维)
  * 双输出头：

    * 均值头：Linear(256→32)
    * 方差头：Linear(256→32)+Softplus
* **推理加速**:

  * 方差小于0.1的点直接分配类别
  * 仅对高方差点进行聚类计算
* **核心创新**:

  * 每个点输出高斯分布参数：

    * 均值向量μ(32维)
    * 方差向量σ(32维)
* **不确定性可视化**:

  * 方差σ\>0.3的点标记为边界点
* **实施细节**:

  * 使用重参数化技巧训练
  * 方差范围限制在[0.01,1.0]

#### （3）部件分割（部件级别）

三维形状部件分割面临两大难点：①具有相同语义标签的部件可能存在巨大几何差异和歧义性；②同类物体的部件数量可能不同。

**VoxSegNet** 

* 英文全称：Voxel Segmentation Network
* 功能：体素级细粒度部件分割
* 输入：稀疏体素化数据(分辨率≤128³)
* 输出：体素级部件标签(0-1掩码)
* 核心模块：

  * SDE模块(空洞率\=1/2/4)
  * 注意力特征聚合层

**表面CRF系统** 

* 英文全称：Surface-based Conditional Random Field
* 功能：多视角标注融合
* 输入：

  * 多视角置信度图(H×W×C)
  * 表面UV映射
* 输出：统一部件标签(N×1)
* 参数：

  * 邻域半径\=5个三角面片
  * 迭代次数\=50

**SyncSpecCNN** 

* 英文全称：Synchronized Spectral CNN
* 功能：非规则形状图卷积
* 输入：

  * 形状图(顶点+边)
  * 拉普拉斯矩阵
* 输出：顶点级部件概率
* 关键技术：

  * 谱扩张卷积核(带宽\=8)
  * 谱变换网络(3层MLP)

**SFCN** 

* 英文全称：Shape Fully Convolutional Network
* 功能：网格部件分割
* 输入：

  * 顶点坐标(N×3)
  * 法向量(N×3)
  * 曲率(N×1)
* 输出：面片级标签(M×1)
* 后处理：

  * 多标签图割(α-expansion)

**CoSegNet** 

* 英文全称：Co-segmentation Network
* 功能：弱监督共分割
* 输入：未标注点云集合({N\_i×3})
* 输出：统一部件标签集
* 损失函数：

  * 群组一致性损失(ε\=0.1)
* 优化网络：

  * 部件优化网络(3层PointNet)

**BAE-NET** 

* 英文全称：Branched AutoEncoder Network
* 功能：无监督共分割
* 输入：单样本点云(N×3)
* 输出：

  * 部件掩码(N×K)
  * 重建点云(N×3)
* 分支结构：

  * K个独立编码器(共享权重)
  * 解码器(对称结构)

**PartNet** 

* 英文全称：Recursive Part Decomposition Network
* 功能：层次部件分割
* 输入：原始点云(N×3)
* 输出：二叉树结构部件分解
* 决策条件：

  * 几何对称性得分
  * 体积比阈值(0.2)

**零样本分组框架** 

* 功能：跨类别部件学习
* 输入：未见类别点云
* 输出：适配后的部件标签
* 关键技术：

  * 局部上下文约束(半径\=0.3m)
  * 对比学习损失(margin\=1.0)

‍
